{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1142c020",
   "metadata": {},
   "source": [
    "## Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd38af38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,) <U400\n",
      "(400,) int64\n",
      "(100,) <U1200\n",
      "(100,) int64\n",
      "(250,) <U2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('rnn-challenge-data.npz', 'rb') as f:\n",
    "    X = np.load(f)\n",
    "    data_x = X['data_x']\n",
    "    data_y = X['data_y']\n",
    "    val_x = X['val_x']\n",
    "    val_y = X['val_y']\n",
    "    test_x = X['test_x']\n",
    "\n",
    "# TRAINING DATA: INPUT (x) AND OUTPUT (y)\n",
    "print(data_x.shape, data_x.dtype)\n",
    "print(data_y.shape, data_y.dtype)\n",
    "\n",
    "# VALIDATION DATA: INPUT (x) AND OUTPUT (y)\n",
    "print(val_x.shape, val_x.dtype)\n",
    "print(val_y.shape, val_y.dtype)\n",
    "\n",
    "# TEST DATA: INPUT (x) ONLY\n",
    "print(test_x.shape, test_x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354bea5",
   "metadata": {},
   "source": [
    "## Encode genome sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e87cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "\n",
    "def encode_genome_sequence(genome_sequence):\n",
    "    # convert string to list of chars\n",
    "    char_array =  np.array(list(genome_sequence))\n",
    "    \n",
    "    # encode characters using one-hot-encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(char_array)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    \n",
    "    # one hot encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    encoded=onehot_encoder.fit_transform(integer_encoded)\n",
    "    return encoded\n",
    "    \n",
    "encoded_x = encode_genome_sequence(data_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856f64e",
   "metadata": {},
   "source": [
    "## Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f4ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label,sequence_length):\n",
    "    # one hot encoding\n",
    "    encoded=np.zeros(5)\n",
    "    encoded[label]=1\n",
    "    #broadcast to length of the input sequence\n",
    "    #label_sequence=np.broadcast_to(encoded.reshape(1,-1),[sequence_length,5])\n",
    "    #encoded=label_sequence.copy()\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d30d03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## now it has the output format (sequence,feature)\n",
    "test=encode_label(data_y[0],400)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2fad0",
   "metadata": {},
   "source": [
    "## Custom Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b24bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomSequenceDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, transform=None, target_transform=None):\n",
    "        self.sequences = x_data\n",
    "        self.labels = y_data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence=self.sequences[idx]\n",
    "        label=self.labels[idx]\n",
    "        if self.transform:\n",
    "            sequence = self.transform(sequence)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label,len(sequence))\n",
    "        # make y as large as x \n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474e7f0",
   "metadata": {},
   "source": [
    "## Create Dataloader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8278a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dataset=CustomSequenceDataset(data_x,data_y,transform = encode_genome_sequence, target_transform= encode_label)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe459067",
   "metadata": {},
   "source": [
    "## Check Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd034bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM Layer expects the format (sequence_length,batch-size,feature_size) or (batch,sequence,feature) if param batch_first\n",
    "x,y=next(iter(trainloader))\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb6a5a",
   "metadata": {},
   "source": [
    "Ok, batch seems to be first, I'll set the parameter batch_first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08ce96",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e72ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        hidden_size=4\n",
    "        ## gets One-Hot-encoded genome element and returns the hidden values\n",
    "        self.lstm = nn.LSTM(input_size=4, hidden_size=hidden_size,batch_first=True)\n",
    "\n",
    "        # Classifier to make prediction from hidden layer\n",
    "        self.classify = nn.Linear(hidden_size, 5)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        lstm_out, _ = self.lstm(sequence.float())\n",
    "        #class_space = self.classify(lstm_out.view(len(sequence), -1))\n",
    "        class_space = self.classify(lstm_out)\n",
    "        logit = F.log_softmax(class_space, dim=1)\n",
    "        return logit\n",
    "    \n",
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fba7e7",
   "metadata": {},
   "source": [
    "## Define Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec52d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.NLLLoss() # Negative Log Likelihood because classification\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95677087",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13c7de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_epochs=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f84bc901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4, iterations:     5] loss: 1.383\n",
      "[epoch 4, iterations:    10] loss: 1.435\n",
      "[epoch 4, iterations:    15] loss: 1.342\n",
      "[epoch 4, iterations:    20] loss: 1.171\n",
      "[epoch 4, iterations:    25] loss: 1.352\n",
      "[epoch 4, iterations:    30] loss: 1.223\n",
      "[epoch 4, iterations:    35] loss: 1.149\n",
      "[epoch 4, iterations:    40] loss: 1.247\n",
      "[epoch 4, iterations:    45] loss: 1.104\n",
      "[epoch 4, iterations:    50] loss: 1.208\n",
      "[epoch 4, iterations:    55] loss: 1.161\n",
      "[epoch 4, iterations:    60] loss: 1.128\n",
      "[epoch 4, iterations:    65] loss: 1.099\n",
      "[epoch 4, iterations:    70] loss: 1.224\n",
      "[epoch 4, iterations:    75] loss: 1.122\n",
      "[epoch 4, iterations:    80] loss: 1.055\n",
      "[epoch 4, iterations:    85] loss: 1.175\n",
      "[epoch 4, iterations:    90] loss: 1.074\n",
      "[epoch 4, iterations:    95] loss: 1.131\n",
      "[epoch 4, iterations:   100] loss: 1.147\n",
      "[epoch 4, iterations:   105] loss: 1.125\n",
      "[epoch 4, iterations:   110] loss: 1.107\n",
      "[epoch 4, iterations:   115] loss: 1.018\n",
      "[epoch 4, iterations:   120] loss: 1.109\n",
      "[epoch 4, iterations:   125] loss: 0.990\n",
      "[epoch 4, iterations:   130] loss: 1.039\n",
      "[epoch 4, iterations:   135] loss: 0.939\n",
      "[epoch 4, iterations:   140] loss: 1.034\n",
      "[epoch 4, iterations:   145] loss: 0.999\n",
      "[epoch 4, iterations:   150] loss: 1.111\n",
      "[epoch 4, iterations:   155] loss: 1.038\n",
      "[epoch 4, iterations:   160] loss: 1.062\n",
      "[epoch 4, iterations:   165] loss: 1.006\n",
      "[epoch 4, iterations:   170] loss: 0.973\n",
      "[epoch 4, iterations:   175] loss: 1.071\n",
      "[epoch 4, iterations:   180] loss: 0.871\n",
      "[epoch 4, iterations:   185] loss: 0.927\n",
      "[epoch 4, iterations:   190] loss: 0.974\n",
      "[epoch 4, iterations:   195] loss: 0.912\n",
      "[epoch 4, iterations:   200] loss: 0.915\n",
      "[epoch 4, iterations:   205] loss: 0.904\n",
      "[epoch 4, iterations:   210] loss: 1.052\n",
      "[epoch 4, iterations:   215] loss: 0.927\n",
      "[epoch 4, iterations:   220] loss: 0.852\n",
      "[epoch 4, iterations:   225] loss: 0.861\n",
      "[epoch 4, iterations:   230] loss: 0.863\n",
      "[epoch 4, iterations:   235] loss: 0.966\n",
      "[epoch 4, iterations:   240] loss: 0.935\n",
      "[epoch 4, iterations:   245] loss: 0.809\n",
      "[epoch 4, iterations:   250] loss: 0.904\n",
      "[epoch 4, iterations:   255] loss: 0.849\n",
      "[epoch 4, iterations:   260] loss: 0.827\n",
      "[epoch 4, iterations:   265] loss: 0.804\n",
      "[epoch 4, iterations:   270] loss: 0.847\n",
      "[epoch 4, iterations:   275] loss: 0.879\n",
      "[epoch 4, iterations:   280] loss: 0.852\n",
      "[epoch 4, iterations:   285] loss: 0.802\n",
      "[epoch 4, iterations:   290] loss: 0.879\n",
      "[epoch 4, iterations:   295] loss: 0.791\n",
      "[epoch 4, iterations:   300] loss: 0.962\n",
      "[epoch 4, iterations:   305] loss: 0.799\n",
      "[epoch 4, iterations:   310] loss: 0.858\n",
      "[epoch 4, iterations:   315] loss: 0.826\n",
      "[epoch 4, iterations:   320] loss: 0.800\n",
      "[epoch 4, iterations:   325] loss: 0.858\n",
      "[epoch 4, iterations:   330] loss: 0.835\n",
      "[epoch 4, iterations:   335] loss: 0.754\n",
      "[epoch 4, iterations:   340] loss: 0.812\n",
      "[epoch 4, iterations:   345] loss: 0.771\n",
      "[epoch 4, iterations:   350] loss: 0.796\n",
      "[epoch 4, iterations:   355] loss: 0.803\n",
      "[epoch 4, iterations:   360] loss: 0.815\n",
      "[epoch 4, iterations:   365] loss: 0.706\n",
      "[epoch 4, iterations:   370] loss: 0.847\n",
      "[epoch 4, iterations:   375] loss: 0.780\n",
      "[epoch 4, iterations:   380] loss: 0.749\n",
      "[epoch 4, iterations:   385] loss: 0.708\n",
      "[epoch 4, iterations:   390] loss: 0.819\n",
      "[epoch 4, iterations:   395] loss: 0.736\n",
      "[epoch 4, iterations:   400] loss: 0.747\n",
      "[epoch 5, iterations:     5] loss: 0.784\n",
      "[epoch 5, iterations:    10] loss: 0.736\n",
      "[epoch 5, iterations:    15] loss: 0.715\n",
      "[epoch 5, iterations:    20] loss: 0.722\n",
      "[epoch 5, iterations:    25] loss: 0.697\n",
      "[epoch 5, iterations:    30] loss: 0.700\n",
      "[epoch 5, iterations:    35] loss: 0.698\n",
      "[epoch 5, iterations:    40] loss: 0.735\n",
      "[epoch 5, iterations:    45] loss: 0.727\n",
      "[epoch 5, iterations:    50] loss: 0.806\n",
      "[epoch 5, iterations:    55] loss: 0.776\n",
      "[epoch 5, iterations:    60] loss: 0.762\n",
      "[epoch 5, iterations:    65] loss: 0.677\n",
      "[epoch 5, iterations:    70] loss: 0.716\n",
      "[epoch 5, iterations:    75] loss: 0.796\n",
      "[epoch 5, iterations:    80] loss: 0.689\n",
      "[epoch 5, iterations:    85] loss: 0.776\n",
      "[epoch 5, iterations:    90] loss: 0.736\n",
      "[epoch 5, iterations:    95] loss: 0.785\n",
      "[epoch 5, iterations:   100] loss: 0.667\n",
      "[epoch 5, iterations:   105] loss: 0.803\n",
      "[epoch 5, iterations:   110] loss: 0.740\n",
      "[epoch 5, iterations:   115] loss: 0.748\n",
      "[epoch 5, iterations:   120] loss: 0.743\n",
      "[epoch 5, iterations:   125] loss: 0.710\n",
      "[epoch 5, iterations:   130] loss: 0.690\n",
      "[epoch 5, iterations:   135] loss: 0.692\n",
      "[epoch 5, iterations:   140] loss: 0.692\n",
      "[epoch 5, iterations:   145] loss: 0.734\n",
      "[epoch 5, iterations:   150] loss: 0.743\n",
      "[epoch 5, iterations:   155] loss: 0.689\n",
      "[epoch 5, iterations:   160] loss: 0.692\n",
      "[epoch 5, iterations:   165] loss: 0.708\n",
      "[epoch 5, iterations:   170] loss: 0.654\n",
      "[epoch 5, iterations:   175] loss: 0.678\n",
      "[epoch 5, iterations:   180] loss: 0.731\n",
      "[epoch 5, iterations:   185] loss: 0.664\n",
      "[epoch 5, iterations:   190] loss: 0.706\n",
      "[epoch 5, iterations:   195] loss: 0.673\n",
      "[epoch 5, iterations:   200] loss: 0.728\n",
      "[epoch 5, iterations:   205] loss: 0.660\n",
      "[epoch 5, iterations:   210] loss: 0.694\n",
      "[epoch 5, iterations:   215] loss: 0.685\n",
      "[epoch 5, iterations:   220] loss: 0.687\n",
      "[epoch 5, iterations:   225] loss: 0.696\n",
      "[epoch 5, iterations:   230] loss: 0.694\n",
      "[epoch 5, iterations:   235] loss: 0.730\n",
      "[epoch 5, iterations:   240] loss: 0.660\n",
      "[epoch 5, iterations:   245] loss: 0.681\n",
      "[epoch 5, iterations:   250] loss: 0.777\n",
      "[epoch 5, iterations:   255] loss: 0.686\n",
      "[epoch 5, iterations:   260] loss: 0.681\n",
      "[epoch 5, iterations:   265] loss: 0.682\n",
      "[epoch 5, iterations:   270] loss: 0.781\n",
      "[epoch 5, iterations:   275] loss: 0.663\n",
      "[epoch 5, iterations:   280] loss: 0.655\n",
      "[epoch 5, iterations:   285] loss: 0.661\n",
      "[epoch 5, iterations:   290] loss: 0.671\n",
      "[epoch 5, iterations:   295] loss: 0.693\n",
      "[epoch 5, iterations:   300] loss: 0.725\n",
      "[epoch 5, iterations:   305] loss: 0.700\n",
      "[epoch 5, iterations:   310] loss: 0.710\n",
      "[epoch 5, iterations:   315] loss: 0.647\n",
      "[epoch 5, iterations:   320] loss: 0.624\n",
      "[epoch 5, iterations:   325] loss: 0.706\n",
      "[epoch 5, iterations:   330] loss: 0.627\n",
      "[epoch 5, iterations:   335] loss: 0.680\n",
      "[epoch 5, iterations:   340] loss: 0.699\n",
      "[epoch 5, iterations:   345] loss: 0.634\n",
      "[epoch 5, iterations:   350] loss: 0.692\n",
      "[epoch 5, iterations:   355] loss: 0.627\n",
      "[epoch 5, iterations:   360] loss: 0.643\n",
      "[epoch 5, iterations:   365] loss: 0.646\n",
      "[epoch 5, iterations:   370] loss: 0.631\n",
      "[epoch 5, iterations:   375] loss: 0.629\n",
      "[epoch 5, iterations:   380] loss: 0.649\n",
      "[epoch 5, iterations:   385] loss: 0.651\n",
      "[epoch 5, iterations:   390] loss: 0.645\n",
      "[epoch 5, iterations:   395] loss: 0.676\n",
      "[epoch 5, iterations:   400] loss: 0.714\n",
      "[epoch 6, iterations:     5] loss: 0.615\n",
      "[epoch 6, iterations:    10] loss: 0.699\n",
      "[epoch 6, iterations:    15] loss: 0.677\n",
      "[epoch 6, iterations:    20] loss: 0.615\n",
      "[epoch 6, iterations:    25] loss: 0.599\n",
      "[epoch 6, iterations:    30] loss: 0.617\n",
      "[epoch 6, iterations:    35] loss: 0.659\n",
      "[epoch 6, iterations:    40] loss: 0.622\n",
      "[epoch 6, iterations:    45] loss: 0.643\n",
      "[epoch 6, iterations:    50] loss: 0.665\n",
      "[epoch 6, iterations:    55] loss: 0.636\n",
      "[epoch 6, iterations:    60] loss: 0.621\n",
      "[epoch 6, iterations:    65] loss: 0.620\n",
      "[epoch 6, iterations:    70] loss: 0.677\n",
      "[epoch 6, iterations:    75] loss: 0.600\n",
      "[epoch 6, iterations:    80] loss: 0.716\n",
      "[epoch 6, iterations:    85] loss: 0.633\n",
      "[epoch 6, iterations:    90] loss: 0.635\n",
      "[epoch 6, iterations:    95] loss: 0.623\n",
      "[epoch 6, iterations:   100] loss: 0.632\n",
      "[epoch 6, iterations:   105] loss: 0.646\n",
      "[epoch 6, iterations:   110] loss: 0.643\n",
      "[epoch 6, iterations:   115] loss: 0.627\n",
      "[epoch 6, iterations:   120] loss: 0.640\n",
      "[epoch 6, iterations:   125] loss: 0.586\n",
      "[epoch 6, iterations:   130] loss: 0.614\n",
      "[epoch 6, iterations:   135] loss: 0.620\n",
      "[epoch 6, iterations:   140] loss: 0.607\n",
      "[epoch 6, iterations:   145] loss: 0.633\n",
      "[epoch 6, iterations:   150] loss: 0.585\n",
      "[epoch 6, iterations:   155] loss: 0.669\n",
      "[epoch 6, iterations:   160] loss: 0.667\n",
      "[epoch 6, iterations:   165] loss: 0.632\n",
      "[epoch 6, iterations:   170] loss: 0.615\n",
      "[epoch 6, iterations:   175] loss: 0.652\n",
      "[epoch 6, iterations:   180] loss: 0.654\n",
      "[epoch 6, iterations:   185] loss: 0.586\n",
      "[epoch 6, iterations:   190] loss: 0.589\n",
      "[epoch 6, iterations:   195] loss: 0.597\n",
      "[epoch 6, iterations:   200] loss: 0.634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6, iterations:   205] loss: 0.594\n",
      "[epoch 6, iterations:   210] loss: 0.614\n",
      "[epoch 6, iterations:   215] loss: 0.628\n",
      "[epoch 6, iterations:   220] loss: 0.594\n",
      "[epoch 6, iterations:   225] loss: 0.570\n",
      "[epoch 6, iterations:   230] loss: 0.607\n",
      "[epoch 6, iterations:   235] loss: 0.592\n",
      "[epoch 6, iterations:   240] loss: 0.598\n",
      "[epoch 6, iterations:   245] loss: 0.593\n",
      "[epoch 6, iterations:   250] loss: 0.651\n",
      "[epoch 6, iterations:   255] loss: 0.581\n",
      "[epoch 6, iterations:   260] loss: 0.607\n",
      "[epoch 6, iterations:   265] loss: 0.594\n",
      "[epoch 6, iterations:   270] loss: 0.646\n",
      "[epoch 6, iterations:   275] loss: 0.607\n",
      "[epoch 6, iterations:   280] loss: 0.615\n",
      "[epoch 6, iterations:   285] loss: 0.620\n",
      "[epoch 6, iterations:   290] loss: 0.606\n",
      "[epoch 6, iterations:   295] loss: 0.578\n",
      "[epoch 6, iterations:   300] loss: 0.618\n",
      "[epoch 6, iterations:   305] loss: 0.614\n",
      "[epoch 6, iterations:   310] loss: 0.589\n",
      "[epoch 6, iterations:   315] loss: 0.604\n",
      "[epoch 6, iterations:   320] loss: 0.582\n",
      "[epoch 6, iterations:   325] loss: 0.602\n",
      "[epoch 6, iterations:   330] loss: 0.574\n",
      "[epoch 6, iterations:   335] loss: 0.600\n",
      "[epoch 6, iterations:   340] loss: 0.579\n",
      "[epoch 6, iterations:   345] loss: 0.596\n",
      "[epoch 6, iterations:   350] loss: 0.611\n",
      "[epoch 6, iterations:   355] loss: 0.608\n",
      "[epoch 6, iterations:   360] loss: 0.561\n",
      "[epoch 6, iterations:   365] loss: 0.633\n",
      "[epoch 6, iterations:   370] loss: 0.613\n",
      "[epoch 6, iterations:   375] loss: 0.642\n",
      "[epoch 6, iterations:   380] loss: 0.609\n",
      "[epoch 6, iterations:   385] loss: 0.611\n",
      "[epoch 6, iterations:   390] loss: 0.601\n",
      "[epoch 6, iterations:   395] loss: 0.631\n",
      "[epoch 6, iterations:   400] loss: 0.620\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "# print every 5th sequence\n",
    "print_running_loss = 5\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    overall_epochs+=1\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        sequences, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        label_scores = lstm(sequences)\n",
    "        loss = loss_fn(label_scores, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_running_loss == print_running_loss-1:    # print 10 times during an epoch\n",
    "            print('[epoch %d, iterations: %5d] loss: %.3f' %\n",
    "                  (overall_epochs + 1, i + 1, running_loss / print_running_loss))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105626ea",
   "metadata": {},
   "source": [
    "## Validate using Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcd0f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "def decode_label(encoded_label):\n",
    "    return argmax(encoded_label)\n",
    "\n",
    "def decode_labels(encoded_labels):\n",
    "    return argmax(encoded_labels[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b485d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "test_dataset=CustomSequenceDataset(val_x,val_y,transform = encode_genome_sequence, target_transform= encode_label)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size)\n",
    "predictions=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader, 0): \n",
    "        sequence,label = data\n",
    "        pred_label = lstm(sequence)\n",
    "        predictions.append(decode_label(pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19f89102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE THAT YOU HAVE THE RIGHT FORMAT\n",
    "assert prediction_test.ndim == 1\n",
    "assert prediction_test.shape[0] == 250\n",
    "\n",
    "# AND SAVE EXACTLY AS SHOWN BELOW\n",
    "np.save('prediction.npy', prediction.astype(int))\n",
    "\n",
    "# MAKE SURE THAT THE FILE HAS THE CORRECT FORMAT\n",
    "def validate_prediction_format():\n",
    "    loaded = np.load('prediction.npy')\n",
    "    assert loaded.shape == (250, )\n",
    "    assert loaded.dtype == int\n",
    "    assert (loaded <= 4).all()\n",
    "    assert (loaded >= 0).all()\n",
    "validate_prediction_format()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchEnv",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
