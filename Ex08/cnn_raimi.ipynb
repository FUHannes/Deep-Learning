{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,) <U400\n",
      "(400,) int64\n",
      "(100,) <U1200\n",
      "(100,) int64\n",
      "(250,) <U2000\n"
     ]
    }
   ],
   "source": [
    "with open('rnn-challenge-data.npz', 'rb') as f:\n",
    "    X = np.load(f)\n",
    "    data_x = X['data_x']\n",
    "    data_y = X['data_y']\n",
    "    val_x = X['val_x']\n",
    "    val_y = X['val_y']\n",
    "    test_x = X['test_x']\n",
    "\n",
    "# TRAINING DATA: INPUT (x) AND OUTPUT (y)\n",
    "print(data_x.shape, data_x.dtype)\n",
    "print(data_y.shape, data_y.dtype)\n",
    "\n",
    "# VALIDATION DATA: INPUT (x) AND OUTPUT (y)\n",
    "print(val_x.shape, val_x.dtype)\n",
    "print(val_y.shape, val_y.dtype)\n",
    "\n",
    "# TEST DATA: INPUT (x) ONLY\n",
    "print(test_x.shape, test_x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def zero_pad_1d(source, target_size = 500):\n",
    "    source_size = source.size()[-1]\n",
    "    pad_size = int((target_size - source_size)/2)\n",
    "    residual = int((target_size - source_size)%2)\n",
    "    return F.pad(input=source, pad=(pad_size,pad_size+residual), mode='constant', value=0)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    \"\"\"Interpret the output of the network, which we know to be a likelihood of each category. \n",
    "    We can use Tensor.topk to get the index of the greatest value:\"\"\"\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def genSeqToTensor(gen_seq):\n",
    "    \"\"\"Turn a sequence of genetic code into a <sequence_length x 1 x n_nucleobases> Tensor.\"\"\"\n",
    "    tensor = torch.zeros(1, len(gen_seq))\n",
    "    for idx, nucleobase in enumerate(gen_seq):\n",
    "        tensor[0,idx] = nucleobaseIndices[nucleobase]\n",
    "    return zero_pad_1d(tensor)\n",
    "\n",
    "nucleobaseIndices = {\n",
    "    \"A\":0,\n",
    "    \"C\":1,\n",
    "    \"G\":2,\n",
    "    \"T\":3\n",
    "}\n",
    "\n",
    "all_nucleobases = list(nucleobaseIndices.keys())\n",
    "n_nucleobases = len(nucleobaseIndices)\n",
    "all_categories = [\"class 0\",\"class 1\",\"class 2\",\"class 3\",\"class 4\"]\n",
    "n_categories = len(all_categories)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenSeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, gen_sequences, labels):\n",
    "        self.gen_sequences = gen_sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gen_seq= self.gen_sequences[idx]\n",
    "        gen_seq_tensor = genSeqToTensor( gen_seq)\n",
    "        label_tensor = self.labels[idx]\n",
    "        label = all_categories[label_tensor]\n",
    "        label_tensor = torch.tensor([label_tensor], dtype=torch.long)\n",
    "        return label, gen_seq, label_tensor, gen_seq_tensor\n",
    "data_ds = GenSeqDataset(data_x, data_y)\n",
    "data_dl = torch.utils.data.DataLoader(data_ds, batch_size=4,\n",
    "                        shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(6, 16 , 5)\n",
    "        self.fc1 = nn.Linear(1952, 120)\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.fc3 = nn.Linear(84, n_categories)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.drop1(F.relu(self.fc1(x)))\n",
    "        x = self.drop2(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.003\n",
      "[1,    20] loss: 0.004\n",
      "[1,    30] loss: 0.003\n",
      "[1,    40] loss: 0.003\n",
      "[1,    50] loss: 0.003\n",
      "[1,    60] loss: 0.003\n",
      "[1,    70] loss: 0.002\n",
      "[1,    80] loss: 0.003\n",
      "[1,    90] loss: 0.003\n",
      "[1,   100] loss: 0.003\n",
      "[2,    10] loss: 0.003\n",
      "[2,    20] loss: 0.002\n",
      "[2,    30] loss: 0.003\n",
      "[2,    40] loss: 0.002\n",
      "[2,    50] loss: 0.002\n",
      "[2,    60] loss: 0.003\n",
      "[2,    70] loss: 0.003\n",
      "[2,    80] loss: 0.002\n",
      "[2,    90] loss: 0.003\n",
      "[2,   100] loss: 0.002\n",
      "[3,    10] loss: 0.003\n",
      "[3,    20] loss: 0.002\n",
      "[3,    30] loss: 0.003\n",
      "[3,    40] loss: 0.002\n",
      "[3,    50] loss: 0.002\n",
      "[3,    60] loss: 0.002\n",
      "[3,    70] loss: 0.002\n",
      "[3,    80] loss: 0.003\n",
      "[3,    90] loss: 0.002\n",
      "[3,   100] loss: 0.002\n",
      "[4,    10] loss: 0.002\n",
      "[4,    20] loss: 0.002\n",
      "[4,    30] loss: 0.002\n",
      "[4,    40] loss: 0.001\n",
      "[4,    50] loss: 0.002\n",
      "[4,    60] loss: 0.002\n",
      "[4,    70] loss: 0.002\n",
      "[4,    80] loss: 0.001\n",
      "[4,    90] loss: 0.001\n",
      "[4,   100] loss: 0.002\n",
      "[5,    10] loss: 0.002\n",
      "[5,    20] loss: 0.002\n",
      "[5,    30] loss: 0.001\n",
      "[5,    40] loss: 0.001\n",
      "[5,    50] loss: 0.002\n",
      "[5,    60] loss: 0.002\n",
      "[5,    70] loss: 0.002\n",
      "[5,    80] loss: 0.003\n",
      "[5,    90] loss: 0.001\n",
      "[5,   100] loss: 0.001\n",
      "[6,    10] loss: 0.001\n",
      "[6,    20] loss: 0.001\n",
      "[6,    30] loss: 0.001\n",
      "[6,    40] loss: 0.001\n",
      "[6,    50] loss: 0.001\n",
      "[6,    60] loss: 0.001\n",
      "[6,    70] loss: 0.001\n",
      "[6,    80] loss: 0.001\n",
      "[6,    90] loss: 0.001\n",
      "[6,   100] loss: 0.001\n",
      "[7,    10] loss: 0.001\n",
      "[7,    20] loss: 0.001\n",
      "[7,    30] loss: 0.001\n",
      "[7,    40] loss: 0.001\n",
      "[7,    50] loss: 0.001\n",
      "[7,    60] loss: 0.001\n",
      "[7,    70] loss: 0.001\n",
      "[7,    80] loss: 0.001\n",
      "[7,    90] loss: 0.001\n",
      "[7,   100] loss: 0.001\n",
      "[8,    10] loss: 0.001\n",
      "[8,    20] loss: 0.001\n",
      "[8,    30] loss: 0.001\n",
      "[8,    40] loss: 0.001\n",
      "[8,    50] loss: 0.001\n",
      "[8,    60] loss: 0.001\n",
      "[8,    70] loss: 0.001\n",
      "[8,    80] loss: 0.001\n",
      "[8,    90] loss: 0.001\n",
      "[8,   100] loss: 0.001\n",
      "[9,    10] loss: 0.000\n",
      "[9,    20] loss: 0.001\n",
      "[9,    30] loss: 0.000\n",
      "[9,    40] loss: 0.001\n",
      "[9,    50] loss: 0.001\n",
      "[9,    60] loss: 0.001\n",
      "[9,    70] loss: 0.001\n",
      "[9,    80] loss: 0.000\n",
      "[9,    90] loss: 0.001\n",
      "[9,   100] loss: 0.001\n",
      "[10,    10] loss: 0.001\n",
      "[10,    20] loss: 0.001\n",
      "[10,    30] loss: 0.001\n",
      "[10,    40] loss: 0.001\n",
      "[10,    50] loss: 0.001\n",
      "[10,    60] loss: 0.001\n",
      "[10,    70] loss: 0.000\n",
      "[10,    80] loss: 0.000\n",
      "[10,    90] loss: 0.000\n",
      "[10,   100] loss: 0.001\n",
      "[11,    10] loss: 0.000\n",
      "[11,    20] loss: 0.001\n",
      "[11,    30] loss: 0.000\n",
      "[11,    40] loss: 0.001\n",
      "[11,    50] loss: 0.001\n",
      "[11,    60] loss: 0.000\n",
      "[11,    70] loss: 0.000\n",
      "[11,    80] loss: 0.001\n",
      "[11,    90] loss: 0.001\n",
      "[11,   100] loss: 0.001\n",
      "[12,    10] loss: 0.000\n",
      "[12,    20] loss: 0.000\n",
      "[12,    30] loss: 0.000\n",
      "[12,    40] loss: 0.001\n",
      "[12,    50] loss: 0.000\n",
      "[12,    60] loss: 0.000\n",
      "[12,    70] loss: 0.001\n",
      "[12,    80] loss: 0.000\n",
      "[12,    90] loss: 0.001\n",
      "[12,   100] loss: 0.001\n",
      "[13,    10] loss: 0.000\n",
      "[13,    20] loss: 0.000\n",
      "[13,    30] loss: 0.000\n",
      "[13,    40] loss: 0.001\n",
      "[13,    50] loss: 0.000\n",
      "[13,    60] loss: 0.001\n",
      "[13,    70] loss: 0.001\n",
      "[13,    80] loss: 0.000\n",
      "[13,    90] loss: 0.001\n",
      "[13,   100] loss: 0.000\n",
      "[14,    10] loss: 0.001\n",
      "[14,    20] loss: 0.002\n",
      "[14,    30] loss: 0.001\n",
      "[14,    40] loss: 0.001\n",
      "[14,    50] loss: 0.001\n",
      "[14,    60] loss: 0.001\n",
      "[14,    70] loss: 0.000\n",
      "[14,    80] loss: 0.000\n",
      "[14,    90] loss: 0.000\n",
      "[14,   100] loss: 0.001\n",
      "[15,    10] loss: 0.000\n",
      "[15,    20] loss: 0.000\n",
      "[15,    30] loss: 0.000\n",
      "[15,    40] loss: 0.000\n",
      "[15,    50] loss: 0.000\n",
      "[15,    60] loss: 0.000\n",
      "[15,    70] loss: 0.001\n",
      "[15,    80] loss: 0.001\n",
      "[15,    90] loss: 0.000\n",
      "[15,   100] loss: 0.000\n",
      "[16,    10] loss: 0.000\n",
      "[16,    20] loss: 0.000\n",
      "[16,    30] loss: 0.000\n",
      "[16,    40] loss: 0.000\n",
      "[16,    50] loss: 0.000\n",
      "[16,    60] loss: 0.000\n",
      "[16,    70] loss: 0.000\n",
      "[16,    80] loss: 0.000\n",
      "[16,    90] loss: 0.000\n",
      "[16,   100] loss: 0.000\n",
      "[17,    10] loss: 0.000\n",
      "[17,    20] loss: 0.000\n",
      "[17,    30] loss: 0.001\n",
      "[17,    40] loss: 0.001\n",
      "[17,    50] loss: 0.001\n",
      "[17,    60] loss: 0.000\n",
      "[17,    70] loss: 0.001\n",
      "[17,    80] loss: 0.001\n",
      "[17,    90] loss: 0.000\n",
      "[17,   100] loss: 0.000\n",
      "[18,    10] loss: 0.000\n",
      "[18,    20] loss: 0.000\n",
      "[18,    30] loss: 0.000\n",
      "[18,    40] loss: 0.001\n",
      "[18,    50] loss: 0.001\n",
      "[18,    60] loss: 0.001\n",
      "[18,    70] loss: 0.000\n",
      "[18,    80] loss: 0.001\n",
      "[18,    90] loss: 0.000\n",
      "[18,   100] loss: 0.000\n",
      "[19,    10] loss: 0.000\n",
      "[19,    20] loss: 0.001\n",
      "[19,    30] loss: 0.000\n",
      "[19,    40] loss: 0.000\n",
      "[19,    50] loss: 0.000\n",
      "[19,    60] loss: 0.000\n",
      "[19,    70] loss: 0.001\n",
      "[19,    80] loss: 0.000\n",
      "[19,    90] loss: 0.000\n",
      "[19,   100] loss: 0.001\n",
      "[20,    10] loss: 0.000\n",
      "[20,    20] loss: 0.000\n",
      "[20,    30] loss: 0.001\n",
      "[20,    40] loss: 0.000\n",
      "[20,    50] loss: 0.000\n",
      "[20,    60] loss: 0.001\n",
      "[20,    70] loss: 0.000\n",
      "[20,    80] loss: 0.001\n",
      "[20,    90] loss: 0.000\n",
      "[20,   100] loss: 0.001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        _,_,labels,inputs = data\n",
    "        labels = torch.flatten(labels, 0)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#PATH = './cifar_net.pth'\n",
    "#torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv1d(1, 6, kernel_size=(5,), stride=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(6, 16, kernel_size=(5,), stride=(1,))\n",
       "  (fc1): Linear(in_features=1952, out_features=120, bias=True)\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (drop2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=84, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/center/.pyenv/versions/3.7.9/envs/dl-pt/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "/home/center/.pyenv/versions/3.7.9/envs/dl-pt/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load validation dataset\n",
    "val_ds = GenSeqDataset(val_x, val_y)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=1,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = int(200 / len(val_ds))\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    \"\"\"Same as train() without the backprop.\"\"\"\n",
    "    return net(line_tensor)\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    for category, gen_seq, category_tensor, gen_seq_tensor in val_dl:\n",
    "        category, gen_seq, category_tensor = category[0], gen_seq[0], category_tensor[0]\n",
    "        output = evaluate(gen_seq_tensor)\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        category_i = all_categories.index(category)\n",
    "        confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(genSeqToTensor(input_line).unsqueeze(0))\n",
    "        return categoryFromOutput(output)[1]\n",
    "\n",
    "prediction = np.fromiter((predict(xi) for xi in test_x), int)\n",
    "validation = np.fromiter((predict(xi) for xi in val_x), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(validation == val_y).sum() / len(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#net = Net()\n",
    "#net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-pt",
   "language": "python",
   "name": "dl-pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
