{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1142c020",
   "metadata": {},
   "source": [
    "## Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd38af38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,) <U400\n",
      "(400,) int64\n",
      "(100,) <U1200\n",
      "(100,) int64\n",
      "(250,) <U2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('rnn-challenge-data.npz', 'rb') as f:\n",
    "    X = np.load(f)\n",
    "    data_x = X['data_x']\n",
    "    data_y = X['data_y']\n",
    "    val_x = X['val_x']\n",
    "    val_y = X['val_y']\n",
    "    test_x = X['test_x']\n",
    "\n",
    "# TRAINING DATA: INPUT (x) AND OUTPUT (y)\n",
    "print(data_x.shape, data_x.dtype)\n",
    "print(data_y.shape, data_y.dtype)\n",
    "\n",
    "# VALIDATION DATA: INPUT (x) AND OUTPUT (y)\n",
    "print(val_x.shape, val_x.dtype)\n",
    "print(val_y.shape, val_y.dtype)\n",
    "\n",
    "# TEST DATA: INPUT (x) ONLY\n",
    "print(test_x.shape, test_x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354bea5",
   "metadata": {},
   "source": [
    "## Encode genome sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e87cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "\n",
    "def encode_genome_sequence(genome_sequence):\n",
    "    # convert string to list of chars\n",
    "    char_array =  np.array(list(genome_sequence))\n",
    "    \n",
    "    # encode characters using one-hot-encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(char_array)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    \n",
    "    # one hot encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    encoded=onehot_encoder.fit_transform(integer_encoded)\n",
    "    return encoded\n",
    "    \n",
    "encoded_x = encode_genome_sequence(data_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856f64e",
   "metadata": {},
   "source": [
    "## Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f4ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label,sequence_length):\n",
    "    # one hot encoding\n",
    "    encoded=np.zeros(5)\n",
    "    encoded[label]=1\n",
    "    #broadcast to length of the input sequence\n",
    "    #label_sequence=np.broadcast_to(encoded.reshape(1,-1),[sequence_length,5])\n",
    "    #encoded=label_sequence.copy()\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3e2780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## now it has the output format (sequence,feature)\n",
    "test=encode_label(data_y[0],400)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2fad0",
   "metadata": {},
   "source": [
    "## Custom Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b24bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomSequenceDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, transform=None, target_transform=None):\n",
    "        self.sequences = x_data\n",
    "        self.labels = y_data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence=self.sequences[idx]\n",
    "        label=self.labels[idx]\n",
    "        if self.transform:\n",
    "            sequence = self.transform(sequence)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label,len(sequence))\n",
    "        # make y as large as x \n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474e7f0",
   "metadata": {},
   "source": [
    "## Create Dataloader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8278a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dataset=CustomSequenceDataset(data_x,data_y,transform = encode_genome_sequence, target_transform= encode_label)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c77ec6",
   "metadata": {},
   "source": [
    "## Check Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe3f347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM Layer expects the format (sequence_length,batch-size,feature_size) or (batch,sequence,feature) if param batch_first\n",
    "x,y=next(iter(trainloader))\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6119b",
   "metadata": {},
   "source": [
    "Ok, batch seems to be first, I'll set the parameter batch_first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08ce96",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e72ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        hidden_size=4\n",
    "        ## gets One-Hot-encoded genome element and returns the hidden values\n",
    "        self.lstm = nn.LSTM(input_size=4, hidden_size=hidden_size,batch_first=True)\n",
    "\n",
    "        # Classifier to make prediction from hidden layer\n",
    "        self.classify = nn.Linear(hidden_size, 5)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        lstm_out, _ = self.lstm(sequence.float())\n",
    "        #class_space = self.classify(lstm_out.view(len(sequence), -1))\n",
    "        class_space = self.classify(lstm_out)\n",
    "        logit = F.log_softmax(class_space, dim=1)\n",
    "        return logit\n",
    "    \n",
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fba7e7",
   "metadata": {},
   "source": [
    "## Define Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec52d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.NLLLoss() # Negative Log Likelihood because classification\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95677087",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13c7de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_epochs=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f84bc901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7, iterations:     5] loss: 0.640\n",
      "[epoch 7, iterations:    10] loss: 0.666\n",
      "[epoch 7, iterations:    15] loss: 0.586\n",
      "[epoch 7, iterations:    20] loss: 0.569\n",
      "[epoch 7, iterations:    25] loss: 0.648\n",
      "[epoch 7, iterations:    30] loss: 0.574\n",
      "[epoch 7, iterations:    35] loss: 0.585\n",
      "[epoch 7, iterations:    40] loss: 0.623\n",
      "[epoch 7, iterations:    45] loss: 0.628\n",
      "[epoch 7, iterations:    50] loss: 0.563\n",
      "[epoch 7, iterations:    55] loss: 0.581\n",
      "[epoch 7, iterations:    60] loss: 0.578\n",
      "[epoch 7, iterations:    65] loss: 0.628\n",
      "[epoch 7, iterations:    70] loss: 0.588\n",
      "[epoch 7, iterations:    75] loss: 0.577\n",
      "[epoch 7, iterations:    80] loss: 0.619\n",
      "[epoch 7, iterations:    85] loss: 0.561\n",
      "[epoch 7, iterations:    90] loss: 0.594\n",
      "[epoch 7, iterations:    95] loss: 0.563\n",
      "[epoch 7, iterations:   100] loss: 0.568\n",
      "[epoch 7, iterations:   105] loss: 0.573\n",
      "[epoch 7, iterations:   110] loss: 0.586\n",
      "[epoch 7, iterations:   115] loss: 0.575\n",
      "[epoch 7, iterations:   120] loss: 0.558\n",
      "[epoch 7, iterations:   125] loss: 0.551\n",
      "[epoch 7, iterations:   130] loss: 0.574\n",
      "[epoch 7, iterations:   135] loss: 0.584\n",
      "[epoch 7, iterations:   140] loss: 0.597\n",
      "[epoch 7, iterations:   145] loss: 0.583\n",
      "[epoch 7, iterations:   150] loss: 0.593\n",
      "[epoch 7, iterations:   155] loss: 0.569\n",
      "[epoch 7, iterations:   160] loss: 0.613\n",
      "[epoch 7, iterations:   165] loss: 0.581\n",
      "[epoch 7, iterations:   170] loss: 0.586\n",
      "[epoch 7, iterations:   175] loss: 0.618\n",
      "[epoch 7, iterations:   180] loss: 0.610\n",
      "[epoch 7, iterations:   185] loss: 0.616\n",
      "[epoch 7, iterations:   190] loss: 0.595\n",
      "[epoch 7, iterations:   195] loss: 0.576\n",
      "[epoch 7, iterations:   200] loss: 0.565\n",
      "[epoch 7, iterations:   205] loss: 0.546\n",
      "[epoch 7, iterations:   210] loss: 0.583\n",
      "[epoch 7, iterations:   215] loss: 0.575\n",
      "[epoch 7, iterations:   220] loss: 0.560\n",
      "[epoch 7, iterations:   225] loss: 0.597\n",
      "[epoch 7, iterations:   230] loss: 0.555\n",
      "[epoch 7, iterations:   235] loss: 0.608\n",
      "[epoch 7, iterations:   240] loss: 0.594\n",
      "[epoch 7, iterations:   245] loss: 0.549\n",
      "[epoch 7, iterations:   250] loss: 0.574\n",
      "[epoch 7, iterations:   255] loss: 0.613\n",
      "[epoch 7, iterations:   260] loss: 0.581\n",
      "[epoch 7, iterations:   265] loss: 0.546\n",
      "[epoch 7, iterations:   270] loss: 0.588\n",
      "[epoch 7, iterations:   275] loss: 0.598\n",
      "[epoch 7, iterations:   280] loss: 0.588\n",
      "[epoch 7, iterations:   285] loss: 0.599\n",
      "[epoch 7, iterations:   290] loss: 0.557\n",
      "[epoch 7, iterations:   295] loss: 0.569\n",
      "[epoch 7, iterations:   300] loss: 0.595\n",
      "[epoch 7, iterations:   305] loss: 0.587\n",
      "[epoch 7, iterations:   310] loss: 0.586\n",
      "[epoch 7, iterations:   315] loss: 0.586\n",
      "[epoch 7, iterations:   320] loss: 0.541\n",
      "[epoch 7, iterations:   325] loss: 0.561\n",
      "[epoch 7, iterations:   330] loss: 0.595\n",
      "[epoch 7, iterations:   335] loss: 0.545\n",
      "[epoch 7, iterations:   340] loss: 0.555\n",
      "[epoch 7, iterations:   345] loss: 0.541\n",
      "[epoch 7, iterations:   350] loss: 0.583\n",
      "[epoch 7, iterations:   355] loss: 0.533\n",
      "[epoch 7, iterations:   360] loss: 0.558\n",
      "[epoch 7, iterations:   365] loss: 0.592\n",
      "[epoch 7, iterations:   370] loss: 0.542\n",
      "[epoch 7, iterations:   375] loss: 0.563\n",
      "[epoch 7, iterations:   380] loss: 0.542\n",
      "[epoch 7, iterations:   385] loss: 0.573\n",
      "[epoch 7, iterations:   390] loss: 0.554\n",
      "[epoch 7, iterations:   395] loss: 0.568\n",
      "[epoch 7, iterations:   400] loss: 0.584\n",
      "[epoch 8, iterations:     5] loss: 0.537\n",
      "[epoch 8, iterations:    10] loss: 0.561\n",
      "[epoch 8, iterations:    15] loss: 0.559\n",
      "[epoch 8, iterations:    20] loss: 0.557\n",
      "[epoch 8, iterations:    25] loss: 0.540\n",
      "[epoch 8, iterations:    30] loss: 0.556\n",
      "[epoch 8, iterations:    35] loss: 0.570\n",
      "[epoch 8, iterations:    40] loss: 0.577\n",
      "[epoch 8, iterations:    45] loss: 0.577\n",
      "[epoch 8, iterations:    50] loss: 0.539\n",
      "[epoch 8, iterations:    55] loss: 0.534\n",
      "[epoch 8, iterations:    60] loss: 0.593\n",
      "[epoch 8, iterations:    65] loss: 0.555\n",
      "[epoch 8, iterations:    70] loss: 0.542\n",
      "[epoch 8, iterations:    75] loss: 0.554\n",
      "[epoch 8, iterations:    80] loss: 0.611\n",
      "[epoch 8, iterations:    85] loss: 0.582\n",
      "[epoch 8, iterations:    90] loss: 0.568\n",
      "[epoch 8, iterations:    95] loss: 0.531\n",
      "[epoch 8, iterations:   100] loss: 0.559\n",
      "[epoch 8, iterations:   105] loss: 0.568\n",
      "[epoch 8, iterations:   110] loss: 0.584\n",
      "[epoch 8, iterations:   115] loss: 0.577\n",
      "[epoch 8, iterations:   120] loss: 0.593\n",
      "[epoch 8, iterations:   125] loss: 0.551\n",
      "[epoch 8, iterations:   130] loss: 0.537\n",
      "[epoch 8, iterations:   135] loss: 0.540\n",
      "[epoch 8, iterations:   140] loss: 0.552\n",
      "[epoch 8, iterations:   145] loss: 0.555\n",
      "[epoch 8, iterations:   150] loss: 0.542\n",
      "[epoch 8, iterations:   155] loss: 0.550\n",
      "[epoch 8, iterations:   160] loss: 0.607\n",
      "[epoch 8, iterations:   165] loss: 0.558\n",
      "[epoch 8, iterations:   170] loss: 0.570\n",
      "[epoch 8, iterations:   175] loss: 0.538\n",
      "[epoch 8, iterations:   180] loss: 0.566\n",
      "[epoch 8, iterations:   185] loss: 0.566\n",
      "[epoch 8, iterations:   190] loss: 0.548\n",
      "[epoch 8, iterations:   195] loss: 0.553\n",
      "[epoch 8, iterations:   200] loss: 0.557\n",
      "[epoch 8, iterations:   205] loss: 0.580\n",
      "[epoch 8, iterations:   210] loss: 0.558\n",
      "[epoch 8, iterations:   215] loss: 0.576\n",
      "[epoch 8, iterations:   220] loss: 0.601\n",
      "[epoch 8, iterations:   225] loss: 0.548\n",
      "[epoch 8, iterations:   230] loss: 0.549\n",
      "[epoch 8, iterations:   235] loss: 0.560\n",
      "[epoch 8, iterations:   240] loss: 0.572\n",
      "[epoch 8, iterations:   245] loss: 0.552\n",
      "[epoch 8, iterations:   250] loss: 0.542\n",
      "[epoch 8, iterations:   255] loss: 0.564\n",
      "[epoch 8, iterations:   260] loss: 0.562\n",
      "[epoch 8, iterations:   265] loss: 0.573\n",
      "[epoch 8, iterations:   270] loss: 0.568\n",
      "[epoch 8, iterations:   275] loss: 0.558\n",
      "[epoch 8, iterations:   280] loss: 0.555\n",
      "[epoch 8, iterations:   285] loss: 0.575\n",
      "[epoch 8, iterations:   290] loss: 0.548\n",
      "[epoch 8, iterations:   295] loss: 0.542\n",
      "[epoch 8, iterations:   300] loss: 0.532\n",
      "[epoch 8, iterations:   305] loss: 0.550\n",
      "[epoch 8, iterations:   310] loss: 0.556\n",
      "[epoch 8, iterations:   315] loss: 0.547\n",
      "[epoch 8, iterations:   320] loss: 0.575\n",
      "[epoch 8, iterations:   325] loss: 0.558\n",
      "[epoch 8, iterations:   330] loss: 0.585\n",
      "[epoch 8, iterations:   335] loss: 0.537\n",
      "[epoch 8, iterations:   340] loss: 0.574\n",
      "[epoch 8, iterations:   345] loss: 0.548\n",
      "[epoch 8, iterations:   350] loss: 0.553\n",
      "[epoch 8, iterations:   355] loss: 0.583\n",
      "[epoch 8, iterations:   360] loss: 0.550\n",
      "[epoch 8, iterations:   365] loss: 0.526\n",
      "[epoch 8, iterations:   370] loss: 0.576\n",
      "[epoch 8, iterations:   375] loss: 0.546\n",
      "[epoch 8, iterations:   380] loss: 0.543\n",
      "[epoch 8, iterations:   385] loss: 0.563\n",
      "[epoch 8, iterations:   390] loss: 0.549\n",
      "[epoch 8, iterations:   395] loss: 0.573\n",
      "[epoch 8, iterations:   400] loss: 0.563\n",
      "[epoch 9, iterations:     5] loss: 0.556\n",
      "[epoch 9, iterations:    10] loss: 0.553\n",
      "[epoch 9, iterations:    15] loss: 0.537\n",
      "[epoch 9, iterations:    20] loss: 0.560\n",
      "[epoch 9, iterations:    25] loss: 0.546\n",
      "[epoch 9, iterations:    30] loss: 0.537\n",
      "[epoch 9, iterations:    35] loss: 0.558\n",
      "[epoch 9, iterations:    40] loss: 0.521\n",
      "[epoch 9, iterations:    45] loss: 0.558\n",
      "[epoch 9, iterations:    50] loss: 0.524\n",
      "[epoch 9, iterations:    55] loss: 0.581\n",
      "[epoch 9, iterations:    60] loss: 0.546\n",
      "[epoch 9, iterations:    65] loss: 0.547\n",
      "[epoch 9, iterations:    70] loss: 0.530\n",
      "[epoch 9, iterations:    75] loss: 0.577\n",
      "[epoch 9, iterations:    80] loss: 0.560\n",
      "[epoch 9, iterations:    85] loss: 0.536\n",
      "[epoch 9, iterations:    90] loss: 0.527\n",
      "[epoch 9, iterations:    95] loss: 0.537\n",
      "[epoch 9, iterations:   100] loss: 0.551\n",
      "[epoch 9, iterations:   105] loss: 0.551\n",
      "[epoch 9, iterations:   110] loss: 0.541\n",
      "[epoch 9, iterations:   115] loss: 0.547\n",
      "[epoch 9, iterations:   120] loss: 0.536\n",
      "[epoch 9, iterations:   125] loss: 0.544\n",
      "[epoch 9, iterations:   130] loss: 0.547\n",
      "[epoch 9, iterations:   135] loss: 0.526\n",
      "[epoch 9, iterations:   140] loss: 0.541\n",
      "[epoch 9, iterations:   145] loss: 0.574\n",
      "[epoch 9, iterations:   150] loss: 0.576\n",
      "[epoch 9, iterations:   155] loss: 0.532\n",
      "[epoch 9, iterations:   160] loss: 0.536\n",
      "[epoch 9, iterations:   165] loss: 0.542\n",
      "[epoch 9, iterations:   170] loss: 0.543\n",
      "[epoch 9, iterations:   175] loss: 0.561\n",
      "[epoch 9, iterations:   180] loss: 0.534\n",
      "[epoch 9, iterations:   185] loss: 0.558\n",
      "[epoch 9, iterations:   190] loss: 0.542\n",
      "[epoch 9, iterations:   195] loss: 0.561\n",
      "[epoch 9, iterations:   200] loss: 0.549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9, iterations:   205] loss: 0.562\n",
      "[epoch 9, iterations:   210] loss: 0.525\n",
      "[epoch 9, iterations:   215] loss: 0.530\n",
      "[epoch 9, iterations:   220] loss: 0.540\n",
      "[epoch 9, iterations:   225] loss: 0.581\n",
      "[epoch 9, iterations:   230] loss: 0.557\n",
      "[epoch 9, iterations:   235] loss: 0.538\n",
      "[epoch 9, iterations:   240] loss: 0.540\n",
      "[epoch 9, iterations:   245] loss: 0.567\n",
      "[epoch 9, iterations:   250] loss: 0.541\n",
      "[epoch 9, iterations:   255] loss: 0.563\n",
      "[epoch 9, iterations:   260] loss: 0.533\n",
      "[epoch 9, iterations:   265] loss: 0.543\n",
      "[epoch 9, iterations:   270] loss: 0.577\n",
      "[epoch 9, iterations:   275] loss: 0.546\n",
      "[epoch 9, iterations:   280] loss: 0.557\n",
      "[epoch 9, iterations:   285] loss: 0.527\n",
      "[epoch 9, iterations:   290] loss: 0.544\n",
      "[epoch 9, iterations:   295] loss: 0.525\n",
      "[epoch 9, iterations:   300] loss: 0.542\n",
      "[epoch 9, iterations:   305] loss: 0.532\n",
      "[epoch 9, iterations:   310] loss: 0.534\n",
      "[epoch 9, iterations:   315] loss: 0.536\n",
      "[epoch 9, iterations:   320] loss: 0.553\n",
      "[epoch 9, iterations:   325] loss: 0.528\n",
      "[epoch 9, iterations:   330] loss: 0.559\n",
      "[epoch 9, iterations:   335] loss: 0.544\n",
      "[epoch 9, iterations:   340] loss: 0.548\n",
      "[epoch 9, iterations:   345] loss: 0.544\n",
      "[epoch 9, iterations:   350] loss: 0.557\n",
      "[epoch 9, iterations:   355] loss: 0.545\n",
      "[epoch 9, iterations:   360] loss: 0.528\n",
      "[epoch 9, iterations:   365] loss: 0.534\n",
      "[epoch 9, iterations:   370] loss: 0.520\n",
      "[epoch 9, iterations:   375] loss: 0.533\n",
      "[epoch 9, iterations:   380] loss: 0.526\n",
      "[epoch 9, iterations:   385] loss: 0.540\n",
      "[epoch 9, iterations:   390] loss: 0.531\n",
      "[epoch 9, iterations:   395] loss: 0.547\n",
      "[epoch 9, iterations:   400] loss: 0.556\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "# print every 5th sequence\n",
    "print_running_loss = 5\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    overall_epochs+=1\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        sequence, label = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        label_score = lstm(sequence)\n",
    "        loss = loss_fn(label_score, label.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_running_loss == print_running_loss-1:    # print 10 times during an epoch\n",
    "            print('[epoch %d, iterations: %5d] loss: %.3f' %\n",
    "                  (overall_epochs + 1, i + 1, running_loss / print_running_loss))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ebe4b",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2fa52973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "def decode_label(encoded_label):\n",
    "    return argmax(encoded_label)\n",
    "\n",
    "def decode_labels(encoded_labels):\n",
    "    return argmax(encoded_labels[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "457943ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "correct_or_wrong=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence,label in trainloader:\n",
    "        pred_label = lstm(sequence)\n",
    "        prediction= decode_label(pred_label)\n",
    "        correct_or_wrong.append(prediction == decode_label(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37609af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1850)\n"
     ]
    }
   ],
   "source": [
    "accuracy=sum(correct_or_wrong)/len(correct_or_wrong)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3fdec65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0081, 0.0077, 0.0059, 0.0062, 0.0070])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp2(pred_label[0][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3bbf21",
   "metadata": {},
   "source": [
    "## Validate using Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb15a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "test_dataset=CustomSequenceDataset(val_x,val_y,transform = encode_genome_sequence, target_transform= encode_label)\n",
    "testloader = torch.utils.data.DataLoader(train_dataset, batch_size)\n",
    "\n",
    "predictions=[]\n",
    "correct_or_wrong=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence,label in testloader:\n",
    "        pred_label = lstm(sequence)\n",
    "        prediction= decode_label(pred_label)\n",
    "        correct_or_wrong.append(prediction == decode_label(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32a8a031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1850)\n"
     ]
    }
   ],
   "source": [
    "accuracy=sum(correct_or_wrong)/len(correct_or_wrong)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE THAT YOU HAVE THE RIGHT FORMAT\n",
    "assert prediction_test.ndim == 1\n",
    "assert prediction_test.shape[0] == 250\n",
    "\n",
    "# AND SAVE EXACTLY AS SHOWN BELOW\n",
    "np.save('prediction.npy', prediction.astype(int))\n",
    "\n",
    "# MAKE SURE THAT THE FILE HAS THE CORRECT FORMAT\n",
    "def validate_prediction_format():\n",
    "    loaded = np.load('prediction.npy')\n",
    "    assert loaded.shape == (250, )\n",
    "    assert loaded.dtype == int\n",
    "    assert (loaded <= 4).all()\n",
    "    assert (loaded >= 0).all()\n",
    "validate_prediction_format()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchEnv",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
