{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1142c020",
   "metadata": {},
   "source": [
    "## Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd38af38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,) <U400\n",
      "(400,) int64\n",
      "(100,) <U1200\n",
      "(100,) int64\n",
      "(250,) <U2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('rnn-challenge-data.npz', 'rb') as f:\n",
    "    X = np.load(f)\n",
    "    data_x = X['data_x']\n",
    "    data_y = X['data_y']\n",
    "    val_x = X['val_x']\n",
    "    val_y = X['val_y']\n",
    "    test_x = X['test_x']\n",
    "\n",
    "# TRAINING DATA: INPUT (x) AND OUTPUT (y)\n",
    "print(data_x.shape, data_x.dtype)\n",
    "print(data_y.shape, data_y.dtype)\n",
    "\n",
    "# VALIDATION DATA: INPUT (x) AND OUTPUT (y)\n",
    "print(val_x.shape, val_x.dtype)\n",
    "print(val_y.shape, val_y.dtype)\n",
    "\n",
    "# TEST DATA: INPUT (x) ONLY\n",
    "print(test_x.shape, test_x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354bea5",
   "metadata": {},
   "source": [
    "## Encode genome sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e87cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "\n",
    "def encode_genome_sequence(genome_sequence):\n",
    "    # convert string to list of chars\n",
    "    char_array =  np.array(list(genome_sequence))\n",
    "    \n",
    "    # encode characters using one-hot-encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(char_array)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    \n",
    "    # one hot encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    encoded=onehot_encoder.fit_transform(integer_encoded)\n",
    "    return encoded\n",
    "    \n",
    "encoded_x = encode_genome_sequence(data_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2fad0",
   "metadata": {},
   "source": [
    "## Custom Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b24bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomSequenceDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, transform=None, target_transform=None):\n",
    "        self.sequences = x_data\n",
    "        self.labels = y_data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence=self.sequences[idx]\n",
    "        label=self.labels[idx]\n",
    "        if self.transform:\n",
    "            sequence = self.transform(sequence)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label,len(sequence))\n",
    "        # make y as large as x \n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474e7f0",
   "metadata": {},
   "source": [
    "## Create Dataloader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8278a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_dataset=CustomSequenceDataset(data_x,data_y,transform = encode_genome_sequence)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08acbde",
   "metadata": {},
   "source": [
    "## Create Dataloader for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d32c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=CustomSequenceDataset(val_x,val_y,transform = encode_genome_sequence)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, 1)\n",
    "train_testloader = torch.utils.data.DataLoader(train_dataset,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c77ec6",
   "metadata": {},
   "source": [
    "## Check Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe3f347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 400, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM Layer expects the format (sequence_length,batch-size,feature_size) or (batch,sequence,feature) if param batch_first\n",
    "x,y=next(iter(trainloader))\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6119b",
   "metadata": {},
   "source": [
    "Ok, batch seems to be first, I'll set the parameter batch_first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08ce96",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e72ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size=8\n",
    "        self.n_layers=2\n",
    "        ## gets One-Hot-encoded genome element and returns the hidden values\n",
    "        self.lstm = nn.LSTM(input_size=4, hidden_size=self.hidden_size,batch_first=True)\n",
    "\n",
    "        # Classifier to make prediction from hidden layer\n",
    "        self.classify = nn.Linear(self.hidden_size, 5)\n",
    "\n",
    "    def forward(self, sequence, hidden_states):\n",
    "        lstm_out, _ = self.lstm(sequence.float(),hidden_states)\n",
    "        class_space = self.classify(lstm_out)\n",
    "        logit = F.log_softmax(class_space, dim=1)\n",
    "        return logit\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        \"\"\" Set hidden states (h,c) to zero. Can be used for initialization \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        h = weight.new(self.n_layers, batch_size, self.hidden_size).zero_()\n",
    "        c= weight.new(self.n_layers, batch_size, self.hidden_size).zero_()\n",
    "        return h,c\n",
    "    \n",
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e881a2b",
   "metadata": {},
   "source": [
    "## Optional: Load weights from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14284636",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/batch-size1/weights-a0.47-e27.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5ee484f85324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/batch-size1/weights-a0.47-e27.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moverall_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Studium/Deep-Learning/Tutorium/pytorchEnv/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Studium/Deep-Learning/Tutorium/pytorchEnv/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Studium/Deep-Learning/Tutorium/pytorchEnv/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/batch-size1/weights-a0.47-e27.pth'"
     ]
    }
   ],
   "source": [
    "lstm.load_state_dict(torch.load('models/batch-size1/weights-a0.47-e27.pth'))\n",
    "lstm.eval()\n",
    "overall_epochs=40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30423a59",
   "metadata": {},
   "source": [
    "## Training and Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2754281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "def decode_label(encoded_label):\n",
    "    return argmax(encoded_label)\n",
    "\n",
    "# Geht nur mit Batch-Size=1\n",
    "def get_accuracy(dataloader,batch_size=1):\n",
    "    predictions=[]\n",
    "    correct_or_wrong=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequence,label in dataloader:\n",
    "            hidden_states=lstm.init_hidden(batch_size)\n",
    "            pred_label = lstm(sequence,hidden_states)\n",
    "            last_label = pred_label[0][len(pred_label[0])-1]\n",
    "            prediction= decode_label(last_label)\n",
    "            predictions.append(prediction)\n",
    "            correct_or_wrong.append(prediction == label)\n",
    "    return sum(correct_or_wrong)/len(correct_or_wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f984f66d",
   "metadata": {},
   "source": [
    "## Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c74b60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    return(get_accuracy(testloader).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fba7e7",
   "metadata": {},
   "source": [
    "## Define Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec52d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.NLLLoss() # Negative Log Likelihood because classification\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e155fa",
   "metadata": {},
   "source": [
    "# Save hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c9f8a70",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-c5c3f50f817f>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-c5c3f50f817f>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    json.dump({batch_size,class(loss_fn)},file)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# create folder\n",
    "import os \n",
    "from datetime import datetime\n",
    "folder_path = \"models/\"+datetime.now().isoformat()+'/'\n",
    "os.mkdir(folder_path)\n",
    "\n",
    "# dump hyperparameters as json\n",
    "#import json \n",
    "#with open(folder_path+'parameters.json','w') as file:\n",
    "#    json.dump({batch_size,class(loss_fn)},file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95677087",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13c7de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_epochs=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f84bc901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 20, iterations:     5] loss: 5.592\n",
      "[epoch 20, iterations:    10] loss: 5.642\n",
      "[epoch 20, iterations:    15] loss: 5.515\n",
      "[epoch 20, iterations:    20] loss: 5.402\n",
      "[epoch 20, iterations:    25] loss: 5.368\n",
      "[epoch 20, iterations:    30] loss: 5.581\n",
      "[epoch 20, iterations:    35] loss: 5.332\n",
      "[epoch 20, iterations:    40] loss: 5.464\n",
      "[epoch 20, iterations:    45] loss: 5.395\n",
      "[epoch 20, iterations:    50] loss: 5.412\n",
      "[epoch 20, iterations:    55] loss: 5.513\n",
      "[epoch 20, iterations:    60] loss: 5.476\n",
      "[epoch 20, iterations:    65] loss: 5.302\n",
      "[epoch 20, iterations:    70] loss: 5.504\n",
      "[epoch 20, iterations:    75] loss: 5.477\n",
      "[epoch 20, iterations:    80] loss: 5.293\n",
      "[epoch 20, iterations:    85] loss: 5.462\n",
      "[epoch 20, iterations:    90] loss: 5.488\n",
      "[epoch 20, iterations:    95] loss: 5.351\n",
      "[epoch 20, iterations:   100] loss: 5.479\n",
      "[epoch 21, iterations:     5] loss: 5.269\n",
      "[epoch 21, iterations:    10] loss: 5.402\n",
      "[epoch 21, iterations:    15] loss: 5.420\n",
      "[epoch 21, iterations:    20] loss: 5.560\n",
      "[epoch 21, iterations:    25] loss: 5.385\n",
      "[epoch 21, iterations:    30] loss: 5.416\n",
      "[epoch 21, iterations:    35] loss: 5.514\n",
      "[epoch 21, iterations:    40] loss: 5.435\n",
      "[epoch 21, iterations:    45] loss: 5.324\n",
      "[epoch 21, iterations:    50] loss: 5.359\n",
      "[epoch 21, iterations:    55] loss: 5.564\n",
      "[epoch 21, iterations:    60] loss: 5.380\n",
      "[epoch 21, iterations:    65] loss: 5.287\n",
      "[epoch 21, iterations:    70] loss: 5.614\n",
      "[epoch 21, iterations:    75] loss: 5.654\n",
      "[epoch 21, iterations:    80] loss: 5.404\n",
      "[epoch 21, iterations:    85] loss: 5.522\n",
      "[epoch 21, iterations:    90] loss: 5.357\n",
      "[epoch 21, iterations:    95] loss: 5.402\n",
      "[epoch 21, iterations:   100] loss: 5.434\n",
      "[epoch 22, iterations:     5] loss: 5.340\n",
      "[epoch 22, iterations:    10] loss: 5.341\n",
      "[epoch 22, iterations:    15] loss: 5.606\n",
      "[epoch 22, iterations:    20] loss: 5.468\n",
      "[epoch 22, iterations:    25] loss: 5.346\n",
      "[epoch 22, iterations:    30] loss: 5.336\n",
      "[epoch 22, iterations:    35] loss: 5.564\n",
      "[epoch 22, iterations:    40] loss: 5.378\n",
      "[epoch 22, iterations:    45] loss: 5.457\n",
      "[epoch 22, iterations:    50] loss: 5.439\n",
      "[epoch 22, iterations:    55] loss: 5.489\n",
      "[epoch 22, iterations:    60] loss: 5.379\n",
      "[epoch 22, iterations:    65] loss: 5.360\n",
      "[epoch 22, iterations:    70] loss: 5.436\n",
      "[epoch 22, iterations:    75] loss: 5.423\n",
      "[epoch 22, iterations:    80] loss: 5.385\n",
      "[epoch 22, iterations:    85] loss: 5.405\n",
      "[epoch 22, iterations:    90] loss: 5.432\n",
      "[epoch 22, iterations:    95] loss: 5.326\n",
      "[epoch 22, iterations:   100] loss: 5.467\n",
      "[epoch 23, iterations:     5] loss: 5.446\n",
      "[epoch 23, iterations:    10] loss: 5.330\n",
      "[epoch 23, iterations:    15] loss: 5.348\n",
      "[epoch 23, iterations:    20] loss: 5.519\n",
      "[epoch 23, iterations:    25] loss: 5.246\n",
      "[epoch 23, iterations:    30] loss: 5.535\n",
      "[epoch 23, iterations:    35] loss: 5.356\n",
      "[epoch 23, iterations:    40] loss: 5.361\n",
      "[epoch 23, iterations:    45] loss: 5.421\n",
      "[epoch 23, iterations:    50] loss: 5.322\n",
      "[epoch 23, iterations:    55] loss: 5.494\n",
      "[epoch 23, iterations:    60] loss: 5.553\n",
      "[epoch 23, iterations:    65] loss: 5.474\n",
      "[epoch 23, iterations:    70] loss: 5.313\n",
      "[epoch 23, iterations:    75] loss: 5.595\n",
      "[epoch 23, iterations:    80] loss: 5.237\n",
      "[epoch 23, iterations:    85] loss: 5.360\n",
      "[epoch 23, iterations:    90] loss: 5.444\n",
      "[epoch 23, iterations:    95] loss: 5.413\n",
      "[epoch 23, iterations:   100] loss: 5.215\n",
      "[epoch 24, iterations:     5] loss: 5.357\n",
      "[epoch 24, iterations:    10] loss: 5.398\n",
      "[epoch 24, iterations:    15] loss: 5.566\n",
      "[epoch 24, iterations:    20] loss: 5.548\n",
      "[epoch 24, iterations:    25] loss: 5.352\n",
      "[epoch 24, iterations:    30] loss: 5.497\n",
      "[epoch 24, iterations:    35] loss: 5.496\n",
      "[epoch 24, iterations:    40] loss: 5.577\n",
      "[epoch 24, iterations:    45] loss: 5.341\n",
      "[epoch 24, iterations:    50] loss: 5.489\n",
      "[epoch 24, iterations:    55] loss: 5.358\n",
      "[epoch 24, iterations:    60] loss: 5.509\n",
      "[epoch 24, iterations:    65] loss: 5.336\n",
      "[epoch 24, iterations:    70] loss: 5.493\n",
      "[epoch 24, iterations:    75] loss: 5.344\n",
      "[epoch 24, iterations:    80] loss: 5.674\n",
      "[epoch 24, iterations:    85] loss: 5.483\n",
      "[epoch 24, iterations:    90] loss: 5.499\n",
      "[epoch 24, iterations:    95] loss: 5.582\n",
      "[epoch 24, iterations:   100] loss: 5.376\n",
      "[epoch 25, iterations:     5] loss: 5.539\n",
      "[epoch 25, iterations:    10] loss: 5.582\n",
      "[epoch 25, iterations:    15] loss: 5.763\n",
      "[epoch 25, iterations:    20] loss: 5.584\n",
      "[epoch 25, iterations:    25] loss: 5.668\n",
      "[epoch 25, iterations:    30] loss: 5.641\n",
      "[epoch 25, iterations:    35] loss: 5.524\n",
      "[epoch 25, iterations:    40] loss: 5.744\n",
      "[epoch 25, iterations:    45] loss: 5.575\n",
      "[epoch 25, iterations:    50] loss: 5.501\n",
      "[epoch 25, iterations:    55] loss: 5.665\n",
      "[epoch 25, iterations:    60] loss: 5.466\n",
      "[epoch 25, iterations:    65] loss: 5.551\n",
      "[epoch 25, iterations:    70] loss: 5.567\n",
      "[epoch 25, iterations:    75] loss: 5.657\n",
      "[epoch 25, iterations:    80] loss: 5.439\n",
      "[epoch 25, iterations:    85] loss: 5.587\n",
      "[epoch 25, iterations:    90] loss: 5.486\n",
      "[epoch 25, iterations:    95] loss: 5.482\n",
      "[epoch 25, iterations:   100] loss: 5.683\n",
      "[epoch 26, iterations:     5] loss: 5.581\n",
      "[epoch 26, iterations:    10] loss: 5.493\n",
      "[epoch 26, iterations:    15] loss: 5.579\n",
      "[epoch 26, iterations:    20] loss: 5.704\n",
      "[epoch 26, iterations:    25] loss: 5.521\n",
      "[epoch 26, iterations:    30] loss: 5.672\n",
      "[epoch 26, iterations:    35] loss: 5.384\n",
      "[epoch 26, iterations:    40] loss: 5.559\n",
      "[epoch 26, iterations:    45] loss: 5.508\n",
      "[epoch 26, iterations:    50] loss: 5.555\n",
      "[epoch 26, iterations:    55] loss: 5.390\n",
      "[epoch 26, iterations:    60] loss: 5.604\n",
      "[epoch 26, iterations:    65] loss: 5.449\n",
      "[epoch 26, iterations:    70] loss: 5.745\n",
      "[epoch 26, iterations:    75] loss: 5.529\n",
      "[epoch 26, iterations:    80] loss: 5.354\n",
      "[epoch 26, iterations:    85] loss: 5.508\n",
      "[epoch 26, iterations:    90] loss: 5.567\n",
      "[epoch 26, iterations:    95] loss: 5.439\n",
      "[epoch 26, iterations:   100] loss: 5.362\n",
      "[epoch 27, iterations:     5] loss: 5.425\n",
      "[epoch 27, iterations:    10] loss: 5.503\n",
      "[epoch 27, iterations:    15] loss: 5.470\n",
      "[epoch 27, iterations:    20] loss: 5.570\n",
      "[epoch 27, iterations:    25] loss: 5.286\n",
      "[epoch 27, iterations:    30] loss: 5.306\n",
      "[epoch 27, iterations:    35] loss: 5.560\n",
      "[epoch 27, iterations:    40] loss: 5.572\n",
      "[epoch 27, iterations:    45] loss: 5.350\n",
      "[epoch 27, iterations:    50] loss: 5.324\n",
      "[epoch 27, iterations:    55] loss: 5.386\n",
      "[epoch 27, iterations:    60] loss: 5.414\n",
      "[epoch 27, iterations:    65] loss: 5.442\n",
      "[epoch 27, iterations:    70] loss: 5.358\n",
      "[epoch 27, iterations:    75] loss: 5.374\n",
      "[epoch 27, iterations:    80] loss: 5.281\n",
      "[epoch 27, iterations:    85] loss: 5.446\n",
      "[epoch 27, iterations:    90] loss: 5.376\n",
      "[epoch 27, iterations:    95] loss: 5.322\n",
      "[epoch 27, iterations:   100] loss: 5.295\n",
      "[epoch 28, iterations:     5] loss: 5.385\n",
      "[epoch 28, iterations:    10] loss: 5.404\n",
      "[epoch 28, iterations:    15] loss: 5.164\n",
      "[epoch 28, iterations:    20] loss: 5.344\n",
      "[epoch 28, iterations:    25] loss: 5.245\n",
      "[epoch 28, iterations:    30] loss: 5.436\n",
      "[epoch 28, iterations:    35] loss: 5.286\n",
      "[epoch 28, iterations:    40] loss: 5.596\n",
      "[epoch 28, iterations:    45] loss: 5.550\n",
      "[epoch 28, iterations:    50] loss: 5.445\n",
      "[epoch 28, iterations:    55] loss: 5.552\n",
      "[epoch 28, iterations:    60] loss: 5.480\n",
      "[epoch 28, iterations:    65] loss: 5.359\n",
      "[epoch 28, iterations:    70] loss: 5.281\n",
      "[epoch 28, iterations:    75] loss: 5.461\n",
      "[epoch 28, iterations:    80] loss: 5.517\n",
      "[epoch 28, iterations:    85] loss: 5.318\n",
      "[epoch 28, iterations:    90] loss: 5.198\n",
      "[epoch 28, iterations:    95] loss: 5.331\n",
      "[epoch 28, iterations:   100] loss: 5.427\n",
      "[epoch 29, iterations:     5] loss: 5.536\n",
      "[epoch 29, iterations:    10] loss: 5.319\n",
      "[epoch 29, iterations:    15] loss: 5.325\n",
      "[epoch 29, iterations:    20] loss: 5.326\n",
      "[epoch 29, iterations:    25] loss: 5.339\n",
      "[epoch 29, iterations:    30] loss: 5.368\n",
      "[epoch 29, iterations:    35] loss: 5.269\n",
      "[epoch 29, iterations:    40] loss: 5.417\n",
      "[epoch 29, iterations:    45] loss: 5.497\n",
      "[epoch 29, iterations:    50] loss: 5.432\n",
      "[epoch 29, iterations:    55] loss: 5.368\n",
      "[epoch 29, iterations:    60] loss: 5.463\n",
      "[epoch 29, iterations:    65] loss: 5.380\n",
      "[epoch 29, iterations:    70] loss: 5.359\n",
      "[epoch 29, iterations:    75] loss: 5.510\n",
      "[epoch 29, iterations:    80] loss: 5.493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 29, iterations:    85] loss: 5.409\n",
      "[epoch 29, iterations:    90] loss: 5.278\n",
      "[epoch 29, iterations:    95] loss: 5.493\n",
      "[epoch 29, iterations:   100] loss: 5.512\n",
      "[epoch 30, iterations:     5] loss: 5.533\n",
      "[epoch 30, iterations:    10] loss: 5.462\n",
      "[epoch 30, iterations:    15] loss: 5.360\n",
      "[epoch 30, iterations:    20] loss: 5.447\n",
      "[epoch 30, iterations:    25] loss: 5.441\n",
      "[epoch 30, iterations:    30] loss: 5.358\n",
      "[epoch 30, iterations:    35] loss: 5.472\n",
      "[epoch 30, iterations:    40] loss: 5.396\n",
      "[epoch 30, iterations:    45] loss: 5.416\n",
      "[epoch 30, iterations:    50] loss: 5.336\n",
      "[epoch 30, iterations:    55] loss: 5.295\n",
      "[epoch 30, iterations:    60] loss: 5.340\n",
      "[epoch 30, iterations:    65] loss: 5.281\n",
      "[epoch 30, iterations:    70] loss: 5.386\n",
      "[epoch 30, iterations:    75] loss: 5.248\n",
      "[epoch 30, iterations:    80] loss: 5.529\n",
      "[epoch 30, iterations:    85] loss: 5.333\n",
      "[epoch 30, iterations:    90] loss: 5.286\n",
      "[epoch 30, iterations:    95] loss: 5.298\n",
      "[epoch 30, iterations:   100] loss: 5.297\n",
      "[epoch 31, iterations:     5] loss: 5.390\n",
      "[epoch 31, iterations:    10] loss: 5.353\n",
      "[epoch 31, iterations:    15] loss: 5.233\n",
      "[epoch 31, iterations:    20] loss: 5.496\n",
      "[epoch 31, iterations:    25] loss: 5.275\n",
      "[epoch 31, iterations:    30] loss: 5.433\n",
      "[epoch 31, iterations:    35] loss: 5.472\n",
      "[epoch 31, iterations:    40] loss: 5.158\n",
      "[epoch 31, iterations:    45] loss: 5.186\n",
      "[epoch 31, iterations:    50] loss: 5.347\n",
      "[epoch 31, iterations:    55] loss: 5.335\n",
      "[epoch 31, iterations:    60] loss: 5.510\n",
      "[epoch 31, iterations:    65] loss: 5.498\n",
      "[epoch 31, iterations:    70] loss: 5.364\n",
      "[epoch 31, iterations:    75] loss: 5.398\n",
      "[epoch 31, iterations:    80] loss: 5.271\n",
      "[epoch 31, iterations:    85] loss: 5.422\n",
      "[epoch 31, iterations:    90] loss: 5.365\n",
      "[epoch 31, iterations:    95] loss: 5.423\n",
      "[epoch 31, iterations:   100] loss: 5.387\n",
      "[epoch 32, iterations:     5] loss: 5.249\n",
      "[epoch 32, iterations:    10] loss: 5.371\n",
      "[epoch 32, iterations:    15] loss: 5.245\n",
      "[epoch 32, iterations:    20] loss: 5.328\n",
      "[epoch 32, iterations:    25] loss: 5.180\n",
      "[epoch 32, iterations:    30] loss: 5.139\n",
      "[epoch 32, iterations:    35] loss: 5.407\n",
      "[epoch 32, iterations:    40] loss: 5.489\n",
      "[epoch 32, iterations:    45] loss: 5.230\n",
      "[epoch 32, iterations:    50] loss: 5.323\n",
      "[epoch 32, iterations:    55] loss: 5.382\n",
      "[epoch 32, iterations:    60] loss: 5.487\n",
      "[epoch 32, iterations:    65] loss: 5.302\n",
      "[epoch 32, iterations:    70] loss: 5.676\n",
      "[epoch 32, iterations:    75] loss: 5.564\n",
      "[epoch 32, iterations:    80] loss: 5.461\n",
      "[epoch 32, iterations:    85] loss: 5.397\n",
      "[epoch 32, iterations:    90] loss: 5.475\n",
      "[epoch 32, iterations:    95] loss: 5.439\n",
      "[epoch 32, iterations:   100] loss: 5.624\n",
      "[epoch 33, iterations:     5] loss: 5.359\n",
      "[epoch 33, iterations:    10] loss: 5.530\n",
      "[epoch 33, iterations:    15] loss: 5.677\n",
      "[epoch 33, iterations:    20] loss: 5.436\n",
      "[epoch 33, iterations:    25] loss: 5.469\n",
      "[epoch 33, iterations:    30] loss: 5.340\n",
      "[epoch 33, iterations:    35] loss: 5.329\n",
      "[epoch 33, iterations:    40] loss: 5.251\n",
      "[epoch 33, iterations:    45] loss: 5.170\n",
      "[epoch 33, iterations:    50] loss: 5.378\n",
      "[epoch 33, iterations:    55] loss: 5.311\n",
      "[epoch 33, iterations:    60] loss: 5.409\n",
      "[epoch 33, iterations:    65] loss: 5.465\n",
      "[epoch 33, iterations:    70] loss: 5.311\n",
      "[epoch 33, iterations:    75] loss: 5.282\n",
      "[epoch 33, iterations:    80] loss: 5.400\n",
      "[epoch 33, iterations:    85] loss: 5.087\n",
      "[epoch 33, iterations:    90] loss: 5.480\n",
      "[epoch 33, iterations:    95] loss: 5.472\n",
      "[epoch 33, iterations:   100] loss: 5.394\n",
      "[epoch 34, iterations:     5] loss: 5.080\n",
      "[epoch 34, iterations:    10] loss: 5.406\n",
      "[epoch 34, iterations:    15] loss: 5.399\n",
      "[epoch 34, iterations:    20] loss: 5.314\n",
      "[epoch 34, iterations:    25] loss: 5.452\n",
      "[epoch 34, iterations:    30] loss: 5.451\n",
      "[epoch 34, iterations:    35] loss: 5.330\n",
      "[epoch 34, iterations:    40] loss: 5.493\n",
      "[epoch 34, iterations:    45] loss: 5.710\n",
      "[epoch 34, iterations:    50] loss: 5.404\n",
      "[epoch 34, iterations:    55] loss: 5.578\n",
      "[epoch 34, iterations:    60] loss: 5.325\n",
      "[epoch 34, iterations:    65] loss: 5.453\n",
      "[epoch 34, iterations:    70] loss: 5.452\n",
      "[epoch 34, iterations:    75] loss: 5.498\n",
      "[epoch 34, iterations:    80] loss: 5.414\n",
      "[epoch 34, iterations:    85] loss: 5.386\n",
      "[epoch 34, iterations:    90] loss: 5.421\n",
      "[epoch 34, iterations:    95] loss: 5.182\n",
      "[epoch 34, iterations:   100] loss: 5.315\n",
      "[epoch 35, iterations:     5] loss: 5.458\n",
      "[epoch 35, iterations:    10] loss: 5.244\n",
      "[epoch 35, iterations:    15] loss: 5.351\n",
      "[epoch 35, iterations:    20] loss: 5.379\n",
      "[epoch 35, iterations:    25] loss: 5.341\n",
      "[epoch 35, iterations:    30] loss: 5.388\n",
      "[epoch 35, iterations:    35] loss: 5.376\n",
      "[epoch 35, iterations:    40] loss: 5.193\n",
      "[epoch 35, iterations:    45] loss: 5.280\n",
      "[epoch 35, iterations:    50] loss: 5.311\n",
      "[epoch 35, iterations:    55] loss: 5.094\n",
      "[epoch 35, iterations:    60] loss: 5.203\n",
      "[epoch 35, iterations:    65] loss: 5.458\n",
      "[epoch 35, iterations:    70] loss: 5.565\n",
      "[epoch 35, iterations:    75] loss: 5.464\n",
      "[epoch 35, iterations:    80] loss: 5.452\n",
      "[epoch 35, iterations:    85] loss: 5.306\n",
      "[epoch 35, iterations:    90] loss: 5.212\n",
      "[epoch 35, iterations:    95] loss: 5.196\n",
      "[epoch 35, iterations:   100] loss: 5.385\n",
      "[epoch 36, iterations:     5] loss: 5.301\n",
      "[epoch 36, iterations:    10] loss: 5.254\n",
      "[epoch 36, iterations:    15] loss: 5.451\n",
      "[epoch 36, iterations:    20] loss: 5.238\n",
      "[epoch 36, iterations:    25] loss: 5.396\n",
      "[epoch 36, iterations:    30] loss: 5.308\n",
      "[epoch 36, iterations:    35] loss: 5.345\n",
      "[epoch 36, iterations:    40] loss: 5.345\n",
      "[epoch 36, iterations:    45] loss: 5.120\n",
      "[epoch 36, iterations:    50] loss: 5.241\n",
      "[epoch 36, iterations:    55] loss: 5.268\n",
      "[epoch 36, iterations:    60] loss: 5.370\n",
      "[epoch 36, iterations:    65] loss: 5.470\n",
      "[epoch 36, iterations:    70] loss: 5.252\n",
      "[epoch 36, iterations:    75] loss: 5.362\n",
      "[epoch 36, iterations:    80] loss: 5.350\n",
      "[epoch 36, iterations:    85] loss: 5.258\n",
      "[epoch 36, iterations:    90] loss: 5.184\n",
      "[epoch 36, iterations:    95] loss: 5.362\n",
      "[epoch 36, iterations:   100] loss: 5.321\n",
      "[epoch 37, iterations:     5] loss: 5.358\n",
      "[epoch 37, iterations:    10] loss: 5.265\n",
      "[epoch 37, iterations:    15] loss: 5.326\n",
      "[epoch 37, iterations:    20] loss: 5.204\n",
      "[epoch 37, iterations:    25] loss: 5.335\n",
      "[epoch 37, iterations:    30] loss: 5.424\n",
      "[epoch 37, iterations:    35] loss: 5.307\n",
      "[epoch 37, iterations:    40] loss: 5.420\n",
      "[epoch 37, iterations:    45] loss: 5.338\n",
      "[epoch 37, iterations:    50] loss: 5.412\n",
      "[epoch 37, iterations:    55] loss: 5.320\n",
      "[epoch 37, iterations:    60] loss: 5.278\n",
      "[epoch 37, iterations:    65] loss: 5.290\n",
      "[epoch 37, iterations:    70] loss: 5.260\n",
      "[epoch 37, iterations:    75] loss: 5.457\n",
      "[epoch 37, iterations:    80] loss: 5.365\n",
      "[epoch 37, iterations:    85] loss: 5.188\n",
      "[epoch 37, iterations:    90] loss: 5.529\n",
      "[epoch 37, iterations:    95] loss: 5.108\n",
      "[epoch 37, iterations:   100] loss: 5.262\n",
      "[epoch 38, iterations:     5] loss: 5.269\n",
      "[epoch 38, iterations:    10] loss: 5.305\n",
      "[epoch 38, iterations:    15] loss: 5.264\n",
      "[epoch 38, iterations:    20] loss: 5.557\n",
      "[epoch 38, iterations:    25] loss: 5.274\n",
      "[epoch 38, iterations:    30] loss: 5.171\n",
      "[epoch 38, iterations:    35] loss: 5.389\n",
      "[epoch 38, iterations:    40] loss: 5.389\n",
      "[epoch 38, iterations:    45] loss: 5.300\n",
      "[epoch 38, iterations:    50] loss: 5.347\n",
      "[epoch 38, iterations:    55] loss: 5.374\n",
      "[epoch 38, iterations:    60] loss: 5.606\n",
      "[epoch 38, iterations:    65] loss: 5.438\n",
      "[epoch 38, iterations:    70] loss: 5.389\n",
      "[epoch 38, iterations:    75] loss: 5.278\n",
      "[epoch 38, iterations:    80] loss: 5.396\n",
      "[epoch 38, iterations:    85] loss: 5.454\n",
      "[epoch 38, iterations:    90] loss: 5.272\n",
      "[epoch 38, iterations:    95] loss: 5.265\n",
      "[epoch 38, iterations:   100] loss: 5.167\n",
      "[epoch 39, iterations:     5] loss: 5.291\n",
      "[epoch 39, iterations:    10] loss: 5.131\n",
      "[epoch 39, iterations:    15] loss: 5.315\n",
      "[epoch 39, iterations:    20] loss: 5.334\n",
      "[epoch 39, iterations:    25] loss: 5.208\n",
      "[epoch 39, iterations:    30] loss: 5.266\n",
      "[epoch 39, iterations:    35] loss: 5.456\n",
      "[epoch 39, iterations:    40] loss: 5.127\n",
      "[epoch 39, iterations:    45] loss: 5.301\n",
      "[epoch 39, iterations:    50] loss: 5.302\n",
      "[epoch 39, iterations:    55] loss: 5.470\n",
      "[epoch 39, iterations:    60] loss: 5.355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 39, iterations:    65] loss: 5.234\n",
      "[epoch 39, iterations:    70] loss: 5.372\n",
      "[epoch 39, iterations:    75] loss: 5.403\n",
      "[epoch 39, iterations:    80] loss: 5.180\n",
      "[epoch 39, iterations:    85] loss: 5.466\n",
      "[epoch 39, iterations:    90] loss: 5.142\n",
      "[epoch 39, iterations:    95] loss: 5.295\n",
      "[epoch 39, iterations:   100] loss: 5.370\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# print every 5th sequence\n",
    "print_running_loss = 5\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, batch in enumerate(trainloader, 0):\n",
    "        # get the inputs; batch is a list of [inputs, labels]\n",
    "        sequences, labels = batch\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # set hidden states to zero after each sequence\n",
    "        hidden_states = lstm.init_hidden(batch_size)\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        label_scores = lstm(sequences,hidden_states)\n",
    "        last_label = label_scores[:,len(label_scores[0])-1] # only the last element of each sequence!\n",
    "        loss = loss_fn(last_label, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_running_loss == print_running_loss-1:    # print 10 times during an epoch\n",
    "            print('[epoch %d, iterations: %5d] loss: %.3f' %\n",
    "                  (overall_epochs, i + 1, running_loss / print_running_loss))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    ## save model every epoch\n",
    "    val_accuracy= validate()\n",
    "    file_name=\"weights-a\"+str(round(val_accuracy,4))+\"-e\"+str(overall_epochs)+\".pth\"\n",
    "    torch.save(lstm.state_dict(), folder_path+str(batch_size)+\"/\"+file_name)\n",
    "    \n",
    "    overall_epochs+=1\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ebe4b",
   "metadata": {},
   "source": [
    "## Training and Test Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b9fae",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37609af5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4375])\n"
     ]
    }
   ],
   "source": [
    "print(get_accuracy(train_testloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3bbf21",
   "metadata": {},
   "source": [
    "## Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d901d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4700])\n"
     ]
    }
   ],
   "source": [
    "print(get_accuracy(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE THAT YOU HAVE THE RIGHT FORMAT\n",
    "assert prediction_test.ndim == 1\n",
    "assert prediction_test.shape[0] == 250\n",
    "\n",
    "# AND SAVE EXACTLY AS SHOWN BELOW\n",
    "np.save('prediction.npy', prediction.astype(int))\n",
    "\n",
    "# MAKE SURE THAT THE FILE HAS THE CORRECT FORMAT\n",
    "def validate_prediction_format():\n",
    "    loaded = np.load('prediction.npy')\n",
    "    assert loaded.shape == (250, )\n",
    "    assert loaded.dtype == int\n",
    "    assert (loaded <= 4).all()\n",
    "    assert (loaded >= 0).all()\n",
    "validate_prediction_format()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchEnv",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
